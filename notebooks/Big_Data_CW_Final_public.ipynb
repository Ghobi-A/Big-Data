{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "pw0Bi9Di4c71"
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNBni1yTPbzS"
   },
   "source": [
    "# Big Data Coursework - Questions\n",
    "\n",
    "## Data Processing and Machine Learning in the Cloud\n",
    "\n",
    "This is the **INM432 Big Data coursework 2023**.\n",
    "This coursework contains extended elements of **theory** and **practice**, mainly around parallelisation of tasks withSpark and a bit about parallel training using TensorFlow.   \n",
    "\n",
    "## Code and Report\n",
    "\n",
    "Your tasks parallelization of tasks in PySpark, extension, evaluation, and theoretical reflection.\n",
    "Please complete and submit the **coding tasks** in a copy of **this notebook**.\n",
    "Write your code in the **indicated cells** and **include** the **output** in the submitted notebook.  \n",
    "Make sure that **your code contains comments** on its **stucture** and explanations of its **purpose**.\n",
    "\n",
    "Provide also a **report** with the **textual answers in a separate document**.  \n",
    "Include **screenshots** from the Google Cloud web interface (don't use the SCREENSHOT function that Google provides, but take a picture of the graphs you see for the VMs) and result tables, as well as written text about the analysis.\n",
    "\n",
    "## Submission\n",
    "\n",
    "Download and submit **your version of this notebook** as an **.ipynb** file and also submit a **shareable link** to your notebook on Colab in your report (created with the Colab 'Share' function) (**and don\u2019t change the online version after submission**).\n",
    "\n",
    "Further, provide your **report as a PDF document**. **State the number of words** in the document at the end. The report should **not have more than 2000 words**.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNWl-w_Lr-e7"
   },
   "source": [
    "## Introduction and Description\n",
    "\n",
    "This coursework focuses on parallelisation and scalability in the cloud with Spark and TesorFlow/Keras.\n",
    "We start with code based on **lessons 3 and 4** of the [*Fast and Lean Data Science*](https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/courses/fast-and-lean-data-science) course by Martin Gorner.\n",
    "The course is based on Tensorflow for data processing and MachineLearning.\n",
    "Tensorflow's data processing approach is somewhat similar to that of Spark, but you don't need to study Tensorflow, just make sure you understand the high-level structure.  \n",
    "What we will do here is **parallelising** **pre-processing**, and **measuring** performance, and we will perform **evaluation** and **analysis** on the cloud performance, as well as **theoretical discussion**.\n",
    "\n",
    "This coursework contains **3 sections**.\n",
    "\n",
    "### Section 0\n",
    "\n",
    "This section just contains some necessary code for setting up the environment. It has no tasks for you (but do read the code and comments).\n",
    "\n",
    "### Section 1\n",
    "Section 1 is about preprocessing a set of image files.\n",
    "We will work with a public dataset \u201cFlowers\u201d (3600 images, 5 classes).\n",
    "This is not a vast dataset, but it keeps the tasks more manageable for development and you can scale up later, if you like.\n",
    "\n",
    "In **'Getting Started'** we will work through the data preprocessing code from *Fast and Lean Data Science* which uses TensorFlow's `tf.data` package.\n",
    "There is no task for you here, but you will need to re-use some of this code later.\n",
    "\n",
    "In **Task 1** you will **parallelise the data preprocessing in Spark**, using Google Cloud (GC) Dataproc.\n",
    "This involves adapting the code from 'Getting Started' to use Spark and running it in the cloud.\n",
    "\n",
    "### Section 2\n",
    "In **Section 2** we are going to **measure the speed of reading data** in the cloud. In **Task  2** we will **paralellize the measuring** of different configurations **using Spark**.\n",
    "\n",
    "### Section 3\n",
    "\n",
    "This section is about the theoretical discussion, based on one paper, in **Task 3**. The answers should be given in the PDF report.\n",
    "\n",
    "### General points\n",
    "\n",
    "For **all coding tasks**, take the **time of the operations** and for the cloud operations, get performance **information from the web interfaces** for your reporting and analysis.\n",
    "\n",
    "The **tasks** are **mostly independent** of each other. The later tasks can mostly be addressed without needing the solution to the earlier ones.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89B27-TGiDNB"
   },
   "source": [
    "# Section 0: Set-up\n",
    "\n",
    "As usual, you need to run the **imports and authentication every time you work with this notebook**. Use the **local Spark** installation for development before you send jobs to the cloud.\n",
    "\n",
    "Read through this section once and **fill in the project ID the first time**, then you can just step straight throught this at the beginning of each session - except for the two authentication cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-_ASxZPjqjM"
   },
   "source": [
    "### Imports\n",
    "\n",
    "We import some **packages that will be needed throughout**.\n",
    "For the **code that runs in the cloud**, we will need **separate import sections** that will need to be partly different from the one below.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "U6rgexPXmY37",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fced36e2-13bc-4e51-c702-08c94fb9949d"
   },
   "source": [
    "import os, sys, math\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import time\n",
    "import datetime\n",
    "import string\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "import pickle"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSDlLsAZh_se"
   },
   "source": [
    "### Cloud and Drive authentication\n",
    "\n",
    "This is for **authenticating with with GCS Google Drive**, so that we can create and use our own buckets and access Dataproc and AI-Platform.\n",
    "\n",
    "This section **starts with the two interactive authentications**.\n",
    "\n",
    "First, we mount Google Drive for persistent local storage and create a directory `DB-CW` thay you can use for this work.\n",
    "Then we'll set up the cloud environment, including a storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HueTPFr-ZH-P",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "138f19a7-a5a9-45e4-9351-63d8eda68eb8"
   },
   "source": [
    "print('Mounting google drive...')\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd \"/content/drive/MyDrive\"\n",
    "!mkdir BD-CW\n",
    "%cd \"/content/drive/MyDrive/BD-CW\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2zNNa2xxBcg"
   },
   "source": [
    "Next, we authenticate with the GCS to enable access to Dataproc and AI-Platform."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nyt_ZnPIRe8K"
   },
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRXGihnX1rAV"
   },
   "source": [
    "It is useful to **create a new Google Cloud project** for this coursework. You can do this on the [GC Console page](https://console.cloud.google.com) by clicking on the entry at the top, right of the *Google Cloud Platform* and choosing *New Project*. **Copy** the **generated project ID** to the next cell. Also **enable billing** and the **Compute, Storage and Dataproc** APIs like we did during the labs.\n",
    "\n",
    "We also specify the **default project and region**. The REGION should be `us-central1` as that seems to be the only one that reliably works with the free credit.\n",
    "This way we don't have to specify this information every time we access the cloud.  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d2GtQXvF2qlS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fcce43e1-a933-422d-d61f-d5c2ee321a72"
   },
   "source": [
    "import os\n",
    "PROJECT = os.environ.get(\"GCP_PROJECT\")  ### USE YOUR GOOGLE CLOUD PROJECT ID HERE. ###\n",
    "!gcloud config set project $PROJECT\n",
    "REGION = 'us-central1'\n",
    "CLUSTER = '{}-cluster'.format(PROJECT)\n",
    "!gcloud config set compute/region $REGION\n",
    "!gcloud config set dataproc/region $REGION\n",
    "\n",
    "!gcloud config list # show some information"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAl4A9PEhPnR"
   },
   "source": [
    "With the cell below, we **create a storage bucket** that we will use later for **global storage**.\n",
    "If the bucket exists you will see a \"ServiceException: 409 ...\", which does not cause any problems.\n",
    "**You must create your own bucket to have write access.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N7zaq0gThQRR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "636cf387-53c0-41d4-d944-57da09c8afc7"
   },
   "source": [
    "BUCKET = 'gs://{}-storage'.format(PROJECT)\n",
    "!gsutil mb $BUCKET"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JrBHayWyuS-"
   },
   "source": [
    "The cell below just **defines some routines for displaying images** that will be **used later**. You can see the code by double-clicking, but you don't need to study this."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MPkvHdAYNt9J"
   },
   "source": [
    "#@title Utility functions for image display **[RUN THIS TO ACTIVATE]** { display-mode: \"form\" }\n",
    "def display_9_images_from_dataset(dataset):\n",
    "  plt.figure(figsize=(13,13))\n",
    "  subplot=331\n",
    "  for i, (image, label) in enumerate(dataset):\n",
    "    plt.subplot(subplot)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image.numpy().astype(np.uint8))\n",
    "    plt.title(str(label.numpy()), fontsize=16)\n",
    "    # plt.title(label.numpy().decode(), fontsize=16)\n",
    "    subplot += 1\n",
    "    if i==8:\n",
    "      break\n",
    "  plt.tight_layout()\n",
    "  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "  plt.show()\n",
    "\n",
    "def display_training_curves(training, validation, title, subplot):\n",
    "  if subplot%10==1: # set up the subplots on the first call\n",
    "    plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
    "    plt.tight_layout()\n",
    "  ax = plt.subplot(subplot)\n",
    "  ax.set_facecolor('#F8F8F8')\n",
    "  ax.plot(training)\n",
    "  ax.plot(validation)\n",
    "  ax.set_title('model '+ title)\n",
    "  ax.set_ylabel(title)\n",
    "  ax.set_xlabel('epoch')\n",
    "  ax.legend(['train', 'valid.'])\n",
    "\n",
    "def dataset_to_numpy_util(dataset, N):\n",
    "    dataset = dataset.batch(N)\n",
    "    for images, labels in dataset:\n",
    "        numpy_images = images.numpy()\n",
    "        numpy_labels = labels.numpy()\n",
    "        break;\n",
    "    return numpy_images, numpy_labels\n",
    "\n",
    "def title_from_label_and_target(label, correct_label):\n",
    "  correct = (label == correct_label)\n",
    "  return \"{} [{}{}{}]\".format(CLASSES[label], str(correct), ', shoud be ' if not correct else '',\n",
    "                              CLASSES[correct_label] if not correct else ''), correct\n",
    "\n",
    "def display_one_flower(image, title, subplot, red=False):\n",
    "    plt.subplot(subplot)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    plt.title(title, fontsize=16, color='red' if red else 'black')\n",
    "    return subplot+1\n",
    "\n",
    "def display_9_images_with_predictions(images, predictions, labels):\n",
    "  subplot=331\n",
    "  plt.figure(figsize=(13,13))\n",
    "  classes = np.argmax(predictions, axis=-1)\n",
    "  for i, image in enumerate(images):\n",
    "    title, correct = title_from_label_and_target(classes[i], labels[i])\n",
    "    subplot = display_one_flower(image, title, subplot, not correct)\n",
    "    if i >= 8:\n",
    "      break;\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "  plt.show()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9gux-oKnH61"
   },
   "source": [
    "### Install Spark locally for quick testing\n",
    "\n",
    "You can use the cell below to **install Spark locally on this Colab VM** (like in the labs), to do quicker small-scale interactive testing. Using Spark in the cloud with **Dataproc is still required for the final version**.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BWbRwh8hnG5h",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3bc3d1ae-1239-4943-c491-e6baf64b0ab6"
   },
   "source": [
    "%cd\n",
    "!apt-get update -qq\n",
    "!apt-get install openjdk-8-jdk-headless -qq >> /dev/null # send any output to null device\n",
    "!tar -xzf \"/content/drive/My Drive/Big_Data/data/spark/spark-3.2.0-bin-hadoop2.7.tgz\" # unpack\n",
    "\n",
    "!pip install -q findspark\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/root/spark-3.2.0-bin-hadoop2.7\"\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "print(pyspark.__version__)\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "print(sc)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvPXiovhi3ZZ"
   },
   "source": [
    "# Section 1: Data pre-processing\n",
    "\n",
    "This section is about the **pre-processing of a dataset** for deep learning.\n",
    "We first look at a ready-made solution using Tensorflow and then we build a implement the same process with Spark.\n",
    "The tasks are about **parallelisation** and **analysis** the performance of the cloud implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EtOSDFbkjgB"
   },
   "source": [
    "## 1.1 Getting started\n",
    "\n",
    "In this section, we get started with the data pre-processing. The code is based on lecture 3 of the 'Fast and Lean Data Science' course.\n",
    "\n",
    "**This code is using the TensorFlow** `tf.data` package, which supports map functions, similar to Spark. Your **task** will be to **re-implement the same approach in Spark**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CedEByREXifP"
   },
   "source": [
    "We start by **setting some variables for the *Flowers* dataset**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d8K6hL_kiWve"
   },
   "source": [
    "GCS_PUBLIC_BUCKET = 'gs://flowers-public/*/*.jpg' # glob  pattern for input files\n",
    "PARTITIONS = 16 # no of partitions we will use later\n",
    "TARGET_SIZE = [192, 192] # target resolution for the images\n",
    "CLASSES = [b'daisy', b'dandelion', b'roses', b'sunflowers', b'tulips']\n",
    "    # labels for the data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXD5l2WhktGk"
   },
   "source": [
    "We **read the image files** from the public GCS bucket that contains the *Flowers* dataset.\n",
    "**TensorFlow** has **functions** to execute glob patterns that we use to calculate the the number of images in total and per partition (rounded up as we cannont deal with parts of images)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nwsZ8X59mu24",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "eacf09f2-d27d-484a-df3e-8d6c8d789042"
   },
   "source": [
    "nb_images = len(tf.io.gfile.glob(GCS_PUBLIC_BUCKET)) # number of images\n",
    "partition_size = math.ceil(1.0 * nb_images / PARTITIONS) # images per partition (float)\n",
    "print(\"GCS_PATTERN matches {} images, to be divided into {} partitions with up to {} images each.\".format(nb_images, PARTITIONS, partition_size))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Map functions\n",
    "\n",
    "In order to read use the images for learning, they need to be **preprocessed** (decoded, resized, cropped, and potentially recompressed).\n",
    "Below are **map functions** for these steps.\n",
    "You **don't need to study** the **internals of these functions** in detail."
   ],
   "metadata": {
    "id": "TbOsPuMEYS4Y"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "T6SfG6W-O3Ua"
   },
   "source": [
    "def decode_jpeg_and_label(filepath):\n",
    "    # extracts the image data and creates a class label, based on the filepath\n",
    "    bits = tf.io.read_file(filepath)\n",
    "    image = tf.image.decode_jpeg(bits)\n",
    "    # parse flower name from containing directory\n",
    "    label = tf.strings.split(tf.expand_dims(filepath, axis=-1), sep='/')\n",
    "    label2 = label.values[-2]\n",
    "    return image, label2\n",
    "\n",
    "def resize_and_crop_image(image, label):\n",
    "    # Resizes and cropd using \"fill\" algorithm:\n",
    "    # always make sure the resulting image is cut out from the source image\n",
    "    # so that it fills the TARGET_SIZE entirely with no black bars\n",
    "    # and a preserved aspect ratio.\n",
    "    w = tf.shape(image)[0]\n",
    "    h = tf.shape(image)[1]\n",
    "    tw = TARGET_SIZE[1]\n",
    "    th = TARGET_SIZE[0]\n",
    "    resize_crit = (w * th) / (h * tw)\n",
    "    image = tf.cond(resize_crit < 1,\n",
    "                    lambda: tf.image.resize(image, [w*tw/w, h*tw/w]), # if true\n",
    "                    lambda: tf.image.resize(image, [w*th/h, h*th/h])  # if false\n",
    "                    )\n",
    "    nw = tf.shape(image)[0]\n",
    "    nh = tf.shape(image)[1]\n",
    "    image = tf.image.crop_to_bounding_box(image, (nw - tw) // 2, (nh - th) // 2, tw, th)\n",
    "    return image, label\n",
    "\n",
    "def recompress_image(image, label):\n",
    "    # this reduces the amount of data, but takes some time\n",
    "    image = tf.cast(image, tf.uint8)\n",
    "    image = tf.image.encode_jpeg(image, optimize_size=True, chroma_downsampling=False)\n",
    "    return image, label"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "With `tf.data`, we can apply decoding and resizing as map functions."
   ],
   "metadata": {
    "id": "fM4pDy_AShJx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dsetFiles = tf.data.Dataset.list_files(GCS_PUBLIC_BUCKET) # This also shuffles the images\n",
    "dsetDecoded = dsetFiles.map(decode_jpeg_and_label)\n",
    "dsetResized = dsetDecoded.map(resize_and_crop_image)"
   ],
   "metadata": {
    "id": "4XVfde0FShwR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also look at some images using the image display function defined above (the one with the hidden code)."
   ],
   "metadata": {
    "id": "1BGiiEHfi7bS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "display_9_images_from_dataset(dsetResized)"
   ],
   "metadata": {
    "id": "9qouACHdjJYk",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "78be6bbd-f14d-4afd-8eb0-dfa3215c882c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JaRNGdocmWHV"
   },
   "source": [
    "Now, let's test continuous reading from the dataset. We can see that reading the first 100 files already takes some time."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-h5oYUC4zrf5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1c2ff9ac-9725-485a-a83a-867bf0973e77"
   },
   "source": [
    "sample_set = dsetResized.batch(10).take(10) # take 10 batches of 10 images for testing\n",
    "for image, label in sample_set:\n",
    "    print(\"Image batch shape {}, {})\".format(image.numpy().shape,\n",
    "        [lbl.decode('utf8') for lbl in label.numpy()]))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoGFH0oz5Z6y"
   },
   "source": [
    "## 1.2 Improving Speed\n",
    "\n",
    "Using individual image files didn't look very fast. The 'Lean and Fast Data Science' course introduced **two techniques to improve the speed**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIxOPCfI5dz3"
   },
   "source": [
    "\n",
    "### Recompress the images\n",
    "By **compressing** the images in the **reduced resolution** we save on the size.\n",
    "This **costs some CPU time** upfront, but **saves network and disk bandwith**, especially when the data are **read multiple times**.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dxSnCKFX6Fv2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7387dc67-c5e4-4e5d-b6ac-93b611417ba9"
   },
   "source": [
    "# This is a quick test to get an idea how long recompressions takes.\n",
    "dataset4 = dsetResized.map(recompress_image)\n",
    "test_set = dataset4.batch(10).take(10)\n",
    "for image, label in test_set:\n",
    "    print(\"Image batch shape {}, {})\".format(image.numpy().shape, [lbl.decode('utf8') for lbl in label.numpy()]))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kT6qI7FZ6KJW"
   },
   "source": [
    "### Write the dataset to TFRecord files\n",
    "\n",
    "By writing **multiple preprocessed samples into a single file**, we can make further speed gains.\n",
    "We distribute the data over **partitions** to facilitate **parallelisation** when the data are used.\n",
    "First we need to **define a location** where we want to put the file."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Be9JCyLk6hyz"
   },
   "source": [
    "GCS_OUTPUT = BUCKET + '/tfrecords-jpeg-192x192-2/flowers'  # prefix for output file names"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsaOcjdM6rJc"
   },
   "source": [
    "Now we can **write the TFRecord files** to the bucket.\n",
    "\n",
    "Running the cell takes some time and **only needs to be done once** or not at all, as you can use the publicly available data for the next few cells. For convenience I have commented out the call to `write_tfrecords` at the end of the next cell. You don't need to run it (it takes some time), but you'll need to use the code below later (but there is no need to study it in detail).\n",
    "\n",
    "There is a **ready-made pre-processed data** versions available here:\n",
    "`gs://flowers-public/tfrecords-jpeg-192x192-2/`, that we can use for testing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "both",
    "id": "oYzgji7agVg4"
   },
   "source": [
    "# functions for writing TFRecord entries\n",
    "# Feature values are always stored as lists, a single data element will be a list of size 1\n",
    "def _bytestring_feature(list_of_bytestrings):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))\n",
    "\n",
    "def _int_feature(list_of_ints): # int64\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))\n",
    "\n",
    "def to_tfrecord(tfrec_filewriter, img_bytes, label): # Create tf data records\n",
    "    class_num = np.argmax(np.array(CLASSES)==label) # 'roses' => 2 (order defined in CLASSES)\n",
    "    one_hot_class = np.eye(len(CLASSES))[class_num]     # [0, 0, 1, 0, 0] for class #2, roses\n",
    "    feature = {\n",
    "        \"image\": _bytestring_feature([img_bytes]), # one image in the list\n",
    "        \"class\": _int_feature([class_num]) #,      # one class in the list\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "def write_tfrecords(GCS_PATTERN,GCS_OUTPUT,partition_size): # write the images to files.\n",
    "    print(\"Writing TFRecords\")\n",
    "    tt0 = time.time()\n",
    "    filenames = tf.data.Dataset.list_files(GCS_PATTERN)\n",
    "    dataset1 = filenames.map(decode_jpeg_and_label)\n",
    "    dataset2 = dataset1.map(resize_and_crop_image)\n",
    "    dataset3 = dataset2.map(recompress_image)\n",
    "    dataset4 = dataset3.batch(partition_size) # partitioning: there will be one \"batch\" of images per file\n",
    "    for partition, (image, label) in enumerate(dataset4):\n",
    "        # batch size used as partition size here\n",
    "        partition_size = image.numpy().shape[0]\n",
    "        # good practice to have the number of records in the filename\n",
    "        filename = GCS_OUTPUT + \"{:02d}-{}.tfrec\".format(partition, partition_size)\n",
    "        # You need to change GCS_OUTPUT to your own bucket to actually create new files\n",
    "        with tf.io.TFRecordWriter(filename) as out_file:\n",
    "            for i in range(partition_size):\n",
    "                example = to_tfrecord(out_file,\n",
    "                                    image.numpy()[i], # re-compressed image: already a byte string\n",
    "                                    label.numpy()[i] #\n",
    "                                    )\n",
    "                out_file.write(example.SerializeToString())\n",
    "        print(\"Wrote file {} containing {} records\".format(filename, partition_size))\n",
    "    print(\"Total time: \"+str(time.time()-tt0))\n",
    "\n",
    "#  write_tfrecords(GCS_PATTERN,GCS_OUTPUT,partition_size) # uncomment to run this cell"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1zyN9txTUuX"
   },
   "source": [
    "### Test the TFRecord files\n",
    "\n",
    "We can now **read from the TFRecord files**. By default, we use the files in the public bucket. Comment out the 1st line of the cell below to use the files written in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4Xp7Z3DlTSBe"
   },
   "source": [
    "GCS_PUBLIC = 'gs://flowers-public/tfrecords-jpeg-192x192-2/'\n",
    "# remove the line above to use your own files that you generated above\n",
    "\n",
    "def read_tfrecord(example):\n",
    "    features = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),  # tf.string = bytestring (not text string)\n",
    "        \"class\": tf.io.FixedLenFeature([], tf.int64) #,   # shape [] means scalar\n",
    "    }\n",
    "    # decode the TFRecord\n",
    "    example = tf.io.parse_single_example(example, features)\n",
    "    image = tf.image.decode_jpeg(example['image'], channels=3)\n",
    "    image = tf.reshape(image, [*TARGET_SIZE, 3])\n",
    "    class_num = example['class']\n",
    "    return image, class_num\n",
    "\n",
    "def load_dataset(filenames):\n",
    "    # read from TFRecords. For optimal performance, read from multiple\n",
    "    # TFRecord files at once and set the option experimental_deterministic = False\n",
    "    # to allow order-altering optimizations.\n",
    "    option_no_order = tf.data.Options()\n",
    "    option_no_order.experimental_deterministic = False\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.with_options(option_no_order)\n",
    "    dataset = dataset.map(read_tfrecord)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "filenames = tf.io.gfile.glob(GCS_OUTPUT + \"*.tfrec\")\n",
    "datasetTfrec = load_dataset(filenames)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RC98hPqGpugm"
   },
   "source": [
    "Let's have a look **if reading from the TFRecord** files is **quicker**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Rsn5vNrdpM4B",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2d3696e6-3c8f-4417-9651-8cae45feb10d"
   },
   "source": [
    "batched_dataset = datasetTfrec.batch(10)\n",
    "sample_set = batched_dataset.take(10)\n",
    "for image, label in sample_set:\n",
    "    print(\"Image batch shape {}, {})\".format(image.numpy().shape, \\\n",
    "                        [str(lbl) for lbl in label.numpy()]))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IHOERkBHqy4"
   },
   "source": [
    "Wow, we have a **massive speed-up**! The repackageing is worthwhile :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n90Cpq_L-fht"
   },
   "source": [
    "## Task 1: Write TFRecord files to the cloud with Spark (40%)\n",
    "\n",
    "Since recompressing and repackaging is very effective, we would like to be able to do it inparallel for large datasets.\n",
    "This is a relatively straightforward case of **parallelisation**.\n",
    "We will **use Spark to implement** the same process as above, but in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCpEv66mgjfQ"
   },
   "source": [
    "### 1a)\tCreate the script (14%)\n",
    "\n",
    "**Re-implement** the pre-processing in Spark, using Spark mechanisms for **distributing** the workload **over multiple machines**.\n",
    "\n",
    "You need to:\n",
    "\n",
    "i) **Copy** over the **mapping functions** (see section 1.1) and **adapt** the resizing and recompression functions **to Spark** (only one argument). (3%)\n",
    "\n",
    "ii) **Replace** the TensorFlow **Dataset objects with RDDs**, starting with an RDD that contains the list of image filenames. (3%)\n",
    "\n",
    "iii) **Sample** the the RDD to a smaller number at an appropriate position in the code. Specify a sampling factor of 0.02 for short tests. (1%)\n",
    "\n",
    "iv) Then **use the functions from above** to write the TFRecord files. (3%)\n",
    "\n",
    "v) The code for **writing to the TFRecord files** needs to be put into a function, that can be applied to every partition with the ['RDD.mapPartitionsWithIndex'](https://spark.apache.org/docs/2.4.8/api/python/pyspark.html#pyspark.RDD.mapPartitionsWithIndex) function.\n",
    "The return value of that function is not used here, but you should return the filename, so that you have a list of the created TFRecord files. (4%)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ocxFfbNQ8Fhm"
   },
   "source": [
    "### CODING TASK ###\n",
    "##Task i)\n",
    "def decode_jpeg_and_label(filepath):#This function passes the filepath, extracts image data and creates a class label from the filepath\n",
    "    # Extract flower name from directory\n",
    "    bits = tf.io.read_file(filepath)\n",
    "    image = tf.image.decode_jpeg(bits)\n",
    "    # parse flower name from containing directory\n",
    "    label = tf.strings.split(tf.expand_dims(filepath, axis=-1), sep='/')\n",
    "    label2 = label.values[-2]\n",
    "    return image, label2\n",
    "\n",
    "def resize_and_crop_image(input_layer):  #Replaced the two argument variables with one input layer and then extracting the data accordingly in the following line.\n",
    "    image, label = input_layer\n",
    "    # Resizes and cropd using \"fill\" algorithm:\n",
    "    # always make sure the resulting image is cut out from the source image\n",
    "    # so that it fills the TARGET_SIZE entirely with no black bars\n",
    "    # and a preserved aspect ratio.\n",
    "    w = tf.shape(image)[0]\n",
    "    h = tf.shape(image)[1]\n",
    "    tw = TARGET_SIZE[1]\n",
    "    th = TARGET_SIZE[0]\n",
    "    resize_crit = (w * th) / (h * tw)\n",
    "    image = tf.cond(resize_crit < 1,\n",
    "                    lambda: tf.image.resize(image, [w*tw/w, h*tw/w]), # if true\n",
    "                    lambda: tf.image.resize(image, [w*th/h, h*th/h])  # if false\n",
    "                    )\n",
    "    nw = tf.shape(image)[0]\n",
    "    nh = tf.shape(image)[1]\n",
    "    image = tf.image.crop_to_bounding_box(image, (nw - tw) // 2, (nh - th) // 2, tw, th)\n",
    "    return image, label\n",
    "\n",
    "def recompress_image(input_layer): #Replaced the two input variable with one input and then deriving the image and label the same way.\n",
    "    image, label = input_layer\n",
    "    # this reduces the amount of data, but takes some time\n",
    "    image = tf.cast(image, tf.uint8)\n",
    "    image = tf.image.encode_jpeg(image, optimize_size=True, chroma_downsampling=False)\n",
    "    return image, label\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Task ii)\n",
    "FILEPATH = tf.io.gfile.glob(GCS_PUBLIC)  # Defined the file path variable\n",
    "RDD = sc.parallelize(FILEPATH)  # Make an RDD\n",
    "\n",
    "# Task iii)\n",
    "SampledRDD = RDD.sample(False, 0.02)  # Sample the RDD\n",
    "\n",
    "# Task iv)\n",
    "RDD_decoded = SampledRDD.map(decode_jpeg_and_label)\n",
    "RDD_resized = RDD_decoded.map(resize_and_crop_image)\n",
    "RDD_recompressed = RDD_resized.map(recompress_image)\n",
    "\n",
    "print(RDD_recompressed.take(2))\n"
   ],
   "metadata": {
    "id": "gecWebdatwPF",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "44fccca4-4516-44e8-a75e-183eec805fe2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Task v)\n",
    "def write_tfrecords_te(index, partition):\n",
    "    filename = GCS_OUTPUT + \"{}.tfrec\".format(index)  # Using the previous write_tf_records function to generate this.\n",
    "    with tf.io.TFRecordWriter(filename) as out_file:\n",
    "        for element in partition:\n",
    "            image = element[0]\n",
    "            label = element[1]\n",
    "            example = to_tfrecord(out_file,\n",
    "                                  image.numpy(),  # Re-compressed image: already a byte string\n",
    "                                  label.numpy()  # , height.numpy()[i], width.numpy()[i]\n",
    "                                  )\n",
    "            out_file.write(example.SerializeToString())\n",
    "    yield filename\n",
    "\n",
    "\n",
    "te = RDD_recompressed.mapPartitionsWithIndex(write_tfrecords_te)\n",
    "print(te.collect())\n"
   ],
   "metadata": {
    "id": "YOdLbYOXtx2U",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3ed10660-e264-4678-ea6e-7ee27dddc43f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHG5TzULkJh1"
   },
   "source": [
    "### 1b)\tTesting (3%)\n",
    "\n",
    "i) Read from the TFRecord Dataset, using `load_dataset` and `display_9_images_from_dataset` to test.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "crKBTwVXLBWm",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "a6125103-ec11-4c6c-c2e2-079921dc3ed8"
   },
   "source": [
    "### CODING TASK ###\n",
    "tf_records = tf.io.gfile.glob(GCS_OUTPUT + \"*.tfrec\")\n",
    "data = load_dataset(tf_records)\n",
    "display_9_images_from_dataset(data)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ggSOXhehVN9"
   },
   "source": [
    "ii) Write your code above into a file using the *cell magic* `%%writefile spark_write_tfrec.py` at the beginning of the file. Then, run the file  locally in Spark.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile spark_write_tfrec.py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pyspark\n",
    "\n",
    "def _bytestring_feature(list_of_bytestrings):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))\n",
    "\n",
    "def _int_feature(list_of_ints): # int64\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))\n",
    "\n",
    "def to_tfrecord(tfrec_filewriter, img_bytes, label): #, height, width):\n",
    "    class_num = np.argmax(np.array(CLASSES)==label) # 'roses' => 2 (order defined in CLASSES)\n",
    "    one_hot_class = np.eye(len(CLASSES))[class_num]     # [0, 0, 1, 0, 0] for class #2, roses\n",
    "    feature = {\n",
    "        \"image\": _bytestring_feature([img_bytes]), # one image in the list\n",
    "        \"class\": _int_feature([class_num]) #,        # one class in the list\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "def decode_jpeg_and_label(filepath):\n",
    "    # extracts the image data and creates a class label, based on the filepath\n",
    "    bits = tf.io.read_file(filepath)\n",
    "    image = tf.image.decode_jpeg(bits)\n",
    "    # parse flower name from containing directory\n",
    "    label = tf.strings.split(tf.expand_dims(filepath, axis=-1), sep='/')\n",
    "    label2 = label.values[-2]\n",
    "    return image, label2\n",
    "\n",
    "def resize_and_crop_image(input_layer):  #Replaced the two input variable with one input and defined image and label underneath.\n",
    "    image, label = input_layer\n",
    "    # Resizes and cropd using \"fill\" algorithm:\n",
    "    # alwimage, label = input_layerays make sure the resulting image is cut out from the source image\n",
    "    # so that it fills the TARGET_SIZE entirely with no black bars\n",
    "    # and a preserved aspect ratio.\n",
    "    w = tf.shape(image)[0]\n",
    "    h = tf.shape(image)[1]\n",
    "    tw = TARGET_SIZE[1]\n",
    "    th = TARGET_SIZE[0]\n",
    "    resize_crit = (w * th) / (h * tw)\n",
    "    image = tf.cond(resize_crit < 1,\n",
    "                    lambda: tf.image.resize(image, [w*tw/w, h*tw/w]), # if true\n",
    "                    lambda: tf.image.resize(image, [w*th/h, h*th/h])  # if false\n",
    "                    )\n",
    "    nw = tf.shape(image)[0]\n",
    "    nh = tf.shape(image)[1]\n",
    "    image = tf.image.crop_to_bounding_box(image, (nw - tw) // 2, (nh - th) // 2, tw, th)\n",
    "    return image, label\n",
    "\n",
    "def recompress_image(input_layer): #Replaced the two input variable with one input and defined image and label underneath.\n",
    "    image, label = input_layer\n",
    "    # this reduces the amount of data, but takes some time\n",
    "    image = tf.cast(image, tf.uint8)\n",
    "    image = tf.image.encode_jpeg(image, optimize_size=True, chroma_downsampling=False)\n",
    "    return image, label\n",
    "\n",
    "def write_tfrecords_modified(index, data_partition):\n",
    "  tfrecord_filename = GCS_OUTPUT + \"{}.tfrec\".format(index)\n",
    "  with tf.io.TFRecordWriter(tfrecord_filename) as tfrecord_file:\n",
    "    for data_element in data_partition:\n",
    "      img = data_element[0]\n",
    "      lbl = data_element[1]\n",
    "      example = to_tfrecord(tfrecord_file,\n",
    "                            img.numpy(), # re-compressed image: already a byte string\n",
    "                            lbl.numpy() #, height.numpy()[i], width.numpy()[i]\n",
    "                            )\n",
    "      tfrecord_file.write(example.SerializeToString())\n",
    "  return [tfrecord_filename]\n",
    "\n",
    "import os\n",
    "PROJECT = os.environ.get(\"GCP_PROJECT\")\n",
    "GCS_PATTERN = 'gs://flowers-public/*/*.jpg' # glob  pattern for input files\n",
    "PARTITIONS = 2 # no of partitions we will use later\n",
    "TARGET_SIZE = [192, 192] # target resolution for the images\n",
    "CLASSES = [b'daisy', b'dandelion', b'roses', b'sunflowers', b'tulips']\n",
    "    # labels for the data (folder names)\n",
    "BUCKET = 'gs://{}-storage'.format(PROJECT)\n",
    "GCS_OUTPUT = BUCKET + '/tfrecords-jpeg-192x192-2/flowers'\n",
    "\n",
    "#Task 1aii\n",
    "\n",
    "# Make into RDD\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "file = tf.io.gfile.glob(GCS_PATTERN)\n",
    "file_RDD = sc.parallelize(file)\n",
    "\n",
    "#Task 1aiii\n",
    "\n",
    "# Sample the RDD with a factor of 0.02 for short tests\n",
    "RDD_Sampled = file_RDD.sample(False,0.02)\n",
    "\n",
    "# Decode the RDD\n",
    "Decoded_RDD = RDD_Sampled.map(decode_jpeg_and_label)\n",
    "\n",
    "# Resize the RDD\n",
    "Resized_RDD = Decoded_RDD.map(resize_and_crop_image)\n",
    "\n",
    "# Recompress the RDD\n",
    "Recompressed_RDD = Resized_RDD.map(recompress_image)\n",
    "\n",
    "Partitioned_RDD = Recompressed_RDD.repartition(PARTITIONS)\n",
    "tf_records_RDD = Partitioned_RDD.mapPartitionsWithIndex(write_tfrecords_modified)\n",
    "tf_records = tf_records_RDD.collect()\n"
   ],
   "metadata": {
    "id": "7kTC9um02ip-",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "77da2e5b-17b3-4138-ed4c-34c90ce2e66c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ts44bFzGw2a"
   },
   "source": [
    "### 1c) Set up a cluster and run the script. (6%)\n",
    "\n",
    "Following the example from the labs, set up a cluster to run PySpark jobs in the cloud. You need to set up so that TensorFlow is installed on all nodes in the cluster.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### i) Single machine cluster\n",
    "Set up a cluster with a single machine using the maximal SSD size (100) and 8 vCPUs.\n",
    "\n",
    "Enable **package installation** by passing a flag `--initialization-actions` with argument `gs://goog-dataproc-initialization-actions-$REGION/python/pip-install.sh` (this is a public script that will read metadata to determine which packages to install).\n",
    "Then, the **packages are specified** by providing a `--metadata` flag with the argument `PIP_PACKAGES=tensorflow==2.4.0`.\n",
    "\n",
    "Note: consider using `PIP_PACKAGES=\"tensorflow numpy\"` or `PIP_PACKAGES=tensorflow` in case an older version of tensorflow is causing issues.\n",
    "\n",
    "When the cluster is running, run your script to check that it works and keep the output cell output. (3%)\n"
   ],
   "metadata": {
    "id": "aRwWRQpn4Rb_"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eOiK8N66crOd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "13beda4b-ae63-4efa-8286-6687c3950856"
   },
   "source": [
    "!gcloud dataproc clusters create $CLUSTER \\\n",
    "--image-version 1.4-ubuntu18 --single-node \\\n",
    "--master-machine-type n1-standard-8 \\\n",
    "--master-boot-disk-type pd-ssd --master-boot-disk-size 100 \\\n",
    "--max-idle 3600s \\\n",
    "--initialization-actions gs://goog-dataproc-initialization-actions-$REGION/python/pip-install.sh \\\n",
    "--metadata PIP_PACKAGES=tensorflow==2.4.0"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the script in the cloud and test the output."
   ],
   "metadata": {
    "id": "SC4BjJ4SBt0g"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zs4B5egy-H6B",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "34338bf3-b012-462a-e70b-2215c029e26f"
   },
   "source": [
    "### CODING TASK ###\n",
    "!gcloud dataproc jobs submit pyspark --cluster $CLUSTER \\spark_write_tfrec.py"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the free credit tier on Google Cloud, there are normally the following **restrictions** on compute machines:\n",
    "- max 100GB of *SSD persistent disk*\n",
    "- max 2000GB of *standard persistent disk*\n",
    "- max 8 *vCPU*s\n",
    "- no GPUs\n",
    "\n",
    "See [here](https://cloud.google.com/free/docs/gcp-free-tier#free-trial) for details\n",
    "The **disks are virtual** disks, where **I/O speed is limited in proportion to the size**, so we should allocate them evenly.\n",
    "This has mainly an effect on the **time the cluster needs to start**, as we are reading the data mainly from the bucket and we are not writing much to disk at all.  \n"
   ],
   "metadata": {
    "id": "FE_IE6cx3jEO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ii) Maximal cluster\n",
    "Use the **largest possible cluster** within these constraints, i.e. **1 master and 7 worker nodes**.\n",
    "Each of them with 1 (virtual) CPU.\n",
    "The master should get the full *SSD* capacity and the 7 worker nodes should get equal shares of the *standard* disk capacity to maximise throughput.\n",
    "\n",
    "Once the cluster is running, test your script. (3%)"
   ],
   "metadata": {
    "id": "RujZLrUb4bIM"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FadP0sQ33oVv",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2d30c641-0cbb-4024-d915-a21d71a3747d"
   },
   "source": [
    "!gcloud dataproc clusters create $CLUSTER \\\n",
    " --image-version 1.4-ubuntu18 \\\n",
    " --num-masters 1 \\\n",
    " --master-machine-type n1-standard-1 \\\n",
    " --master-boot-disk-type pd-ssd --master-boot-disk-size 100 \\\n",
    "  --num-workers 7 \\\n",
    "  --worker-machine-type n1-standard-1 \\\n",
    "  --worker-boot-disk-size 285 \\\n",
    "  --max-idle 3600s \\\n",
    "  --initialization-actions gs://goog-dataproc-initialization-actions-$REGION/python/pip-install.sh \\\n",
    "  --metadata PIP_PACKAGES=tensorflow==2.4.0"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "### CODING TASK ###\n",
    "!gcloud dataproc jobs submit pyspark --cluster $CLUSTER \\spark_write_tfrec.py"
   ],
   "metadata": {
    "id": "eF3Fc1yG4m2F",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "01d8e0c6-a8fd-4c23-93b6-7a4749539c55"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUQXJ_c-uU2s"
   },
   "source": [
    "### 1d)\tOptimisation, experiments, and discussion (17%)\n",
    "\n",
    "i) Improve parallelisation\n",
    "\n",
    "If you implemented a straightfoward version, you will\n",
    "**probably** observe that **all the computation** is done on only **two nodes**.\n",
    "This can be adressed by using the **second parameter** in the initial call to **parallelize**.\n",
    "Make the **suitable change** in the code you have written above and mark it up in comments as `### TASK 1d ###`.\n",
    "\n",
    "Demonstrate the difference in cluster utilisation before and after the change based on different parameter values with **screenshots from Google Cloud** and measure the **difference in the processing time**. (6%)\n",
    "\n",
    "ii) Experiment with cluster configurations.\n",
    "\n",
    "In addition to the experiments above (using 8 VMs),test your program with 4 machines with double the resources each (2 vCPUs, memory, disk) and 1 machine with eightfold resources.\n",
    "Discuss the results in terms of disk I/O and network bandwidth allocation in the cloud. (7%)\n",
    "\n",
    "iii) Explain the difference between this use of Spark and most standard applications like e.g. in our labs in terms of where the data is stored. What kind of parallelisation approach is used here? (4%)\n",
    "\n",
    "Write the code below and your answers in the report.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile spark_write_tfrec_16_partitions.py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pyspark\n",
    "\n",
    "def _bytestring_feature(list_of_bytestrings):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))\n",
    "\n",
    "def _int_feature(list_of_ints): # int64\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))\n",
    "\n",
    "def to_tfrecord(tfrec_filewriter, img_bytes, label): #, height, width):\n",
    "    class_num = np.argmax(np.array(CLASSES)==label) # 'roses' => 2 (order defined in CLASSES)\n",
    "    one_hot_class = np.eye(len(CLASSES))[class_num]     # [0, 0, 1, 0, 0] for class #2, roses\n",
    "    feature = {\n",
    "        \"image\": _bytestring_feature([img_bytes]), # one image in the list\n",
    "        \"class\": _int_feature([class_num]) #,        # one class in the list\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "def decode_jpeg_and_label(filepath):\n",
    "    # extracts the image data and creates a class label, based on the filepath\n",
    "    bits = tf.io.read_file(filepath)\n",
    "    image = tf.image.decode_jpeg(bits)\n",
    "    # parse flower name from containing directory\n",
    "    label = tf.strings.split(tf.expand_dims(filepath, axis=-1), sep='/')\n",
    "    label2 = label.values[-2]\n",
    "    return image, label2\n",
    "\n",
    "def resize_and_crop_image(input_layer):  #Replaced the two input variable with one input and defined image and label underneath.\n",
    "    image, label = input_layer\n",
    "    # Resizes and cropd using \"fill\" algorithm:\n",
    "    # alwimage, label = input_layerays make sure the resulting image is cut out from the source image\n",
    "    # so that it fills the TARGET_SIZE entirely with no black bars\n",
    "    # and a preserved aspect ratio.\n",
    "    w = tf.shape(image)[0]\n",
    "    h = tf.shape(image)[1]\n",
    "    tw = TARGET_SIZE[1]\n",
    "    th = TARGET_SIZE[0]\n",
    "    resize_crit = (w * th) / (h * tw)\n",
    "    image = tf.cond(resize_crit < 1,\n",
    "                    lambda: tf.image.resize(image, [w*tw/w, h*tw/w]), # if true\n",
    "                    lambda: tf.image.resize(image, [w*th/h, h*th/h])  # if false\n",
    "                    )\n",
    "    nw = tf.shape(image)[0]\n",
    "    nh = tf.shape(image)[1]\n",
    "    image = tf.image.crop_to_bounding_box(image, (nw - tw) // 2, (nh - th) // 2, tw, th)\n",
    "    return image, label\n",
    "\n",
    "def recompress_image(input_layer): #Replaced the two input variable with one input and defined image and label underneath.\n",
    "    image, label = input_layer\n",
    "    # this reduces the amount of data, but takes some time\n",
    "    image = tf.cast(image, tf.uint8)\n",
    "    image = tf.image.encode_jpeg(image, optimize_size=True, chroma_downsampling=False)\n",
    "    return image, label\n",
    "\n",
    "def write_tfrecords_modified(index, data_partition):\n",
    "  tfrecord_filename = GCS_OUTPUT + \"{}.tfrec\".format(index)\n",
    "  with tf.io.TFRecordWriter(tfrecord_filename) as tfrecord_file:\n",
    "    for data_element in data_partition:\n",
    "      img = data_element[0]\n",
    "      lbl = data_element[1]\n",
    "      example = to_tfrecord(tfrecord_file,\n",
    "                            img.numpy(), # re-compressed image: already a byte string\n",
    "                            lbl.numpy() #, height.numpy()[i], width.numpy()[i]\n",
    "                            )\n",
    "      tfrecord_file.write(example.SerializeToString())\n",
    "  return [tfrecord_filename]\n",
    "\n",
    "import os\n",
    "PROJECT = os.environ.get(\"GCP_PROJECT\")\n",
    "GCS_PATTERN = 'gs://flowers-public/*/*.jpg' # glob  pattern for input files\n",
    "PARTITIONS = 16 # no of partitions we will use later\n",
    "TARGET_SIZE = [192, 192] # target resolution for the images\n",
    "CLASSES = [b'daisy', b'dandelion', b'roses', b'sunflowers', b'tulips']\n",
    "    # labels for the data (folder names)\n",
    "BUCKET = 'gs://{}-storage'.format(PROJECT)\n",
    "GCS_OUTPUT = BUCKET + '/tfrecords-jpeg-192x192-2/flowers'\n",
    "\n",
    "#Task 1aii\n",
    "\n",
    "# Make into RDD\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "file = tf.io.gfile.glob(GCS_PATTERN)\n",
    "file_RDD = sc.parallelize(file,16)\n",
    "\n",
    "#Task 1aiii\n",
    "\n",
    "# Sample the RDD with a factor of 0.02 for short tests\n",
    "RDD_Sampled = file_RDD.sample(False,0.02)\n",
    "\n",
    "# Decode the RDD\n",
    "Decoded_RDD = RDD_Sampled.map(decode_jpeg_and_label)\n",
    "\n",
    "# Resize the RDD\n",
    "Resized_RDD = Decoded_RDD.map(resize_and_crop_image)\n",
    "\n",
    "# Recompress the RDD\n",
    "Recompressed_RDD = Resized_RDD.map(recompress_image)\n",
    "\n",
    "tf_records_RDD = Recompressed_RDD.mapPartitionsWithIndex(write_tfrecords_modified)\n",
    "tf_records = tf_records_RDD.collect()\n"
   ],
   "metadata": {
    "id": "8qXDNUfwSirs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "55daaef5-92ae-4a5c-d8cf-27b8c649ab7d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!gcloud dataproc clusters create $CLUSTER \\\n",
    "--image-version 1.4-ubuntu18 \\\n",
    "--num-masters 1 \\\n",
    "--master-machine-type n1-standard-2 \\\n",
    "--master-boot-disk-type pd-ssd --master-boot-disk-size 100 \\\n",
    "--num-workers 3 \\\n",
    "--worker-machine-type n1-standard-2 \\\n",
    "--worker-boot-disk-size 666 \\\n",
    "--max-idle 3600s\\\n",
    "--initialization-actions gs://goog-dataproc-initialization-actions-$REGION/python/pip-install.sh \\\n",
    "--metadata PIP_PACKAGES=tensorflow==2.4.0"
   ],
   "metadata": {
    "id": "Q1Y9J6Y2SraL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e196113d-7d7a-4cbb-83f2-6d8f073f2291"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!gcloud dataproc jobs submit pyspark --cluster $CLUSTER \\spark_write_tfrec_16_partitions.py"
   ],
   "metadata": {
    "id": "kyik7QGiSpb2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ca173011-952c-4a7e-cdc7-84f316c74f78"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xx00RcJNkS-f"
   },
   "source": [
    "# Section 2: Speed tests\n",
    "\n",
    "We have seen that **reading from the pre-processed TFRecord files** is **faster** than reading individual image files and decoding on the fly.\n",
    "This task is about **measuring this effect** and **parallelizing the tests with PySpark**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pw0Bi9Di4c71"
   },
   "source": [
    "## 2.1 Speed test implementation\n",
    "\n",
    "Here is **code for time measurement** to determine the **throughput in images per second**.\n",
    "It doesn't render the images but extracts and prints some basic information in order to make sure the image data are read.\n",
    "We write the information to the null device for longer measurements `null_file=open(\"/dev/null\", mode='w')`.\n",
    "That way it will not clutter our cell output.\n",
    "\n",
    "We use batches ( `dset2 = dset1.batch(batch_size)` ) and select a number of batches with (`dset3 = dset2.take(batch_number)`).\n",
    "Then we  use the `time.time()` to take the **time measurement** and take it multiple times, reading from the same dataset to see if reading speed changes with mutiple readings.\n",
    "\n",
    "We then **vary** the size of the batch (`batch_size`) and the number of batches (`batch_number`) and **store the results for different values**.\n",
    "Store also the **results for each repetition** over the same dataset (repeat 2 or 3 times).\n",
    "\n",
    "The speed test should be combined in a **function** `time_configs()` that takes a configuration, i.e. a dataset and arrays of `batch_sizes`, `batch_numbers`, and  `repetitions` (an array of integers starting from 1), as **arguments** and runs the time measurement for each combination of batch_size and batch_number for the requested number of repetitions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DZlwZop5tsqs"
   },
   "source": [
    "# Here are some useful values for testing your code, use higher values later for actually testing throughput\n",
    "batch_sizes = [2,4]\n",
    "batch_numbers = [3,6]\n",
    "repetitions = [1]\n",
    "\n",
    "def time_configs(dataset, batch_sizes, batch_numbers, repetitions):\n",
    "    dims = [len(batch_sizes),len(batch_numbers),len(repetitions)]\n",
    "    print(dims)\n",
    "    results = np.zeros(dims)\n",
    "    params = np.zeros(dims + [3])\n",
    "    print( results.shape )\n",
    "    with open(\"/dev/null\",mode='w') as null_file: # for printing the output without showing it\n",
    "        tt = time.time() # for overall time taking\n",
    "        for bsi,bs in enumerate(batch_sizes):\n",
    "            for dsi, ds in enumerate(batch_numbers):\n",
    "                batched_dataset = dataset.batch(bs)\n",
    "                timing_set = batched_dataset.take(ds)\n",
    "                for ri,rep in enumerate(repetitions):\n",
    "                    print(\"bs: {}, ds: {}, rep: {}\".format(bs,ds,rep))\n",
    "                    t0 = time.time()\n",
    "                    for image, label in timing_set:\n",
    "                        #print(\"Image batch shape {}\".format(image.numpy().shape),\n",
    "                        print(\"Image batch shape {}, {})\".format(image.numpy().shape,\n",
    "                            [str(lbl) for lbl in label.numpy()]), null_file)\n",
    "                    td = time.time() - t0 # duration for reading images\n",
    "                    results[bsi,dsi,ri] = ( bs * ds) / td\n",
    "                    params[bsi,dsi,ri] = [ bs, ds, rep ]\n",
    "    print(\"total time: \"+str(time.time()-tt))\n",
    "    return results, params"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IICIJYaiVsLz"
   },
   "source": [
    "**Let's try this function** with a **small number** of configurations of batch_sizes batch_numbers and repetions, so that we get a set of parameter combinations and corresponding reading speeds.\n",
    "Try reading from the image files (dataset4) and the TFRecord files (datasetTfrec)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0SwjzQtfj-PE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2bdca12b-78ec-4963-8789-fe89f7c82109"
   },
   "source": [
    "[res,par] = time_configs(dataset4, batch_sizes, batch_numbers, repetitions)\n",
    "print(res)\n",
    "print(par)\n",
    "\n",
    "print(\"=============\")\n",
    "\n",
    "[res,par] = time_configs(datasetTfrec, batch_sizes, batch_numbers, repetitions)\n",
    "print(res)\n",
    "print(par)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQEjuE-bD5W9"
   },
   "source": [
    "## Task 2: Parallelising the speed test with Spark in the cloud. (36%)\n",
    "\n",
    "As an exercise in **Spark programming and optimisation** as well as **performance analysis**, we will now implement the **speed test** with multiple parameters in parallel with Spark.\n",
    "Runing multiple tests in parallel would **not be a useful approach on a single machine, but it can be in the cloud** (you will be asked to reason about this later).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fJFroJzVal-"
   },
   "source": [
    "### 2a) Create the script (14%)\n",
    "Your task is now to **port the speed test above to Spark** for running it in the cloud in Dataproc.\n",
    "**Adapt the speed testing** as a Spark program that performs the same actions as above, but **with Spark RDDs in a distributed way**.\n",
    "The distribution should be such that **each parameter combination (except repetition)** is processed in a separate Spark task.\n",
    "\n",
    "More specifically:\n",
    "*   i) combine the previous cells to have the code to create a dataset and create a list of parameter combinations in an RDD (2%)\n",
    "*   ii) get a Spark context and  create the dataset and run timing test for each combination in parallel (2%)\n",
    "*   iii) transform the resulting RDD to the structure \\( parameter_combination, images_per_second \\) and save these values in an array (2%)\n",
    "*   iv) create an RDD with all results for each parameter as (parameter_value,images_per_second) and collect the result for each parameter  (2%)\n",
    "*   v) create an RDD with the average reading speeds for each parameter value and collect the results. Keep associativity in mind when implementing the average. (3%)\n",
    "*   vi) write the results to a pickle file in your bucket (2%)\n",
    "*   vii) Write your code it into a file using the *cell magic* `%%writefile spark_job.py` (1%)\n",
    "\n",
    "\n",
    "**Important:** The task here is not to parallelize the pre-processing, but to run multiple speed tests in parallel using Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### TFrec"
   ],
   "metadata": {
    "id": "WpoTXBajvH3-"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NP2LEeKMGv4v"
   },
   "source": [
    "### CODING TASK\n",
    "#Task 2ai\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "\n",
    "GCS_PUBLIC_BUCKET = 'gs://flowers-public/*/*.jpg' # glob  pattern for input files\n",
    "PARTITIONS = 16 # no of partitions we will use later\n",
    "TARGET_SIZE = [192, 192] # target resolution for the images\n",
    "CLASSES = [b'daisy', b'dandelion', b'roses', b'sunflowers', b'tulips']\n",
    "    # labels for the data\n",
    "\n",
    "def read_tfrecord(example):\n",
    "    features = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"class\": tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    # decode the TFRecord\n",
    "    example = tf.io.parse_single_example(example, features)\n",
    "    image = tf.image.decode_jpeg(example['image'], channels=3)\n",
    "    image = tf.reshape(image, [*TARGET_SIZE, 3])\n",
    "    class_num = example['class']\n",
    "    return image, class_num\n",
    "\n",
    "def load_dataset(filenames):\n",
    "    # read from TFRecords. For optimal performance, read from multiple\n",
    "    # TFRecord files at once and set the option experimental_deterministic = False\n",
    "    # to allow order-altering optimizations.\n",
    "    option_no_order = tf.data.Options()\n",
    "    option_no_order.experimental_deterministic = False\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.with_options(option_no_order)\n",
    "    dataset = dataset.map(read_tfrecord)\n",
    "    return dataset\n",
    "\n",
    "def load_dataset_decoded():\n",
    "  dataset_filename = tf.data.Dataset.list_files(GCS_PATTERN) #Image Files\n",
    "  datasetDecoded = dataset_filename.map(decode_jpeg_and_label)\n",
    "  datasetfn = datasetDecoded.map(resize_and_crop_image)\n",
    "  return datasetfn\n",
    "\n",
    "def decode_jpeg_and_label(filepath):\n",
    "    bits = tf.io.read_file(filepath)\n",
    "    image = tf.image.decode_jpeg(bits)\n",
    "    label = tf.strings.split(tf.expand_dims(filepath, axis=-1), sep='/')\n",
    "    label2 = label.values[-2]\n",
    "    return image, label2\n",
    "\n",
    "def resize_and_crop_image(input_layer):\n",
    "    image, label = input_layer\n",
    "    w = tf.shape(image)[0]\n",
    "    h = tf.shape(image)[1]\n",
    "    tw = TARGET_SIZE[1]\n",
    "    th = TARGET_SIZE[0]\n",
    "    resize_crit = (w * th) / (h * tw)\n",
    "    image = tf.cond(resize_crit < 1,\n",
    "                    lambda: tf.image.resize(image, [w*tw/w, h*tw/w]), # if true\n",
    "                    lambda: tf.image.resize(image, [w*th/h, h*th/h])  # if false\n",
    "                    )\n",
    "    nw = tf.shape(image)[0]\n",
    "    nh = tf.shape(image)[1]\n",
    "    image = tf.image.crop_to_bounding_box(image, (nw - tw) // 2, (nh - th) // 2, tw, th)\n",
    "    return image, label\n",
    "\n",
    "batch_sizes = [2,4,6,8]\n",
    "batch_numbers = [3,6,12,18]\n",
    "repetitions=[1,2,3]\n",
    "\n",
    "parameter_list = [[i,j,k] for i in batch_sizes for j in batch_numbers for k in repetitions]\n",
    "\n",
    "#Task 2aii\n",
    "sc=pyspark.SparkContext.getOrCreate()\n",
    "RDD_parameter = sc.parallelize(parameter_list, 16) #creates an RDD\n",
    "\n",
    "def time_configs_combined(parameters_rdd):\n",
    "  batch_size, batch_count, repetition_count = parameters_rdd\n",
    "\n",
    "  filenames = tf.io.gfile.glob(GCS_PUBLIC_BUCKET + \"*.tfrec\")\n",
    "  dataset = load_dataset(filenames)\n",
    "\n",
    "  batched_dataset = dataset.batch(batch_size)\n",
    "  sample_dataset = batched_dataset.take(batch_count)\n",
    "\n",
    "  timings = []\n",
    "  for rep in range(repetition_count):\n",
    "    start_time = time.time()\n",
    "    for picture in sample_dataset:\n",
    "      print('string', file=open(\"/dev/null\", mode='w'))\n",
    "    end_time = time.time()\n",
    "    reading_duration = end_time - start_time\n",
    "    throughput = float((batch_size * batch_count) / reading_duration)\n",
    "    dataset_size = batch_size * batch_count\n",
    "    timings.append([batch_size, batch_count, repetition_count, dataset_size, throughput])\n",
    "  return timings\n",
    "\n",
    "#Task 2aiii\n",
    "tfrec_rdd = RDD_parameter.map(time_configs_combined)\n",
    "### TASK 2c ###\n",
    "tfrec_rdd.cache()\n",
    "\n",
    "#Task 2aiv\n",
    "# For TFRec dataset\n",
    "tfrec_images_per_second = tfrec_rdd.flatMap(lambda x: [(str(x[0][0])+'_'+str(x[0][1])+'_'+str(x[0][2]), x[0][4])])\n",
    "### TASK 2c ###\n",
    "tfrec_images_per_second.cache()\n",
    "tfrec_results = tfrec_images_per_second.collect()\n",
    "\n",
    "\n",
    "#Task 2av)\n",
    "# For TFRec dataset\n",
    "tfrec_images_per_second_avg = tfrec_images_per_second \\\n",
    "    .mapValues(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \\\n",
    "    .mapValues(lambda x: x[0] / x[1])\n",
    "tfrec_avg_results = tfrec_images_per_second_avg.collect()\n",
    "\n",
    "\n",
    "#Task 2avi)\n",
    "# Define your bucket name and destination file name\n",
    "import os\n",
    "PROJECT = os.environ.get(\"GCP_PROJECT\")\n",
    "destination_blob_name = \"average_results_tfrec.pkl\"\n",
    "\n",
    "def upload_to_bucket(BUCKET, destination_blob_name, data):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket('{}-storage'.format(PROJECT))\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    # Write the data to a temporary pickle file\n",
    "    with blob.open('wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "# Upload the results to the bucket\n",
    "upload_to_bucket(PROJECT, destination_blob_name, tfrec_avg_results)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### TFRec File"
   ],
   "metadata": {
    "id": "ZqMKre768xok"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Task 2avii)\n",
    "\n",
    "%%writefile spark_job_tfrec.py\n",
    "### CODING TASK\n",
    "#Task 2ai\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from google.cloud import storage\n",
    "import time\n",
    "\n",
    "GCS_PUBLIC_BUCKET = 'gs://flowers-public/*/*.jpg' # glob  pattern for input files\n",
    "PARTITIONS = 16 # no of partitions we will use later\n",
    "TARGET_SIZE = [192, 192] # target resolution for the images\n",
    "CLASSES = [b'daisy', b'dandelion', b'roses', b'sunflowers', b'tulips']\n",
    "    # labels for the data\n",
    "\n",
    "\n",
    "def read_tfrecord(example):\n",
    "    features = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"class\": tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    # decode the TFRecord\n",
    "    example = tf.io.parse_single_example(example, features)\n",
    "    image = tf.image.decode_jpeg(example['image'], channels=3)\n",
    "    image = tf.reshape(image, [*TARGET_SIZE, 3])\n",
    "    class_num = example['class']\n",
    "    return image, class_num\n",
    "\n",
    "def load_dataset(filenames):\n",
    "    # read from TFRecords. For optimal performance, read from multiple\n",
    "    # TFRecord files at once and set the option experimental_deterministic = False\n",
    "    # to allow order-altering optimizations.\n",
    "    option_no_order = tf.data.Options()\n",
    "    option_no_order.experimental_deterministic = False\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.with_options(option_no_order)\n",
    "    dataset = dataset.map(read_tfrecord)\n",
    "    return dataset\n",
    "\n",
    "def load_dataset_decoded():\n",
    "  dataset_filename = tf.data.Dataset.list_files(GCS_PATTERN) #Image Files\n",
    "  datasetDecoded = dataset_filename.map(decode_jpeg_and_label)\n",
    "  datasetfn = datasetDecoded.map(resize_and_crop_image)\n",
    "  return datasetfn\n",
    "\n",
    "def decode_jpeg_and_label(filepath):\n",
    "    bits = tf.io.read_file(filepath)\n",
    "    image = tf.image.decode_jpeg(bits)\n",
    "    label = tf.strings.split(tf.expand_dims(filepath, axis=-1), sep='/')\n",
    "    label2 = label.values[-2]\n",
    "    return image, label2\n",
    "\n",
    "def resize_and_crop_image(input_layer):\n",
    "    image, label = input_layer\n",
    "    w = tf.shape(image)[0]\n",
    "    h = tf.shape(image)[1]\n",
    "    tw = TARGET_SIZE[1]\n",
    "    th = TARGET_SIZE[0]\n",
    "    resize_crit = (w * th) / (h * tw)\n",
    "    image = tf.cond(resize_crit < 1,\n",
    "                    lambda: tf.image.resize(image, [w*tw/w, h*tw/w]), # if true\n",
    "                    lambda: tf.image.resize(image, [w*th/h, h*th/h])  # if false\n",
    "                    )\n",
    "    nw = tf.shape(image)[0]\n",
    "    nh = tf.shape(image)[1]\n",
    "    image = tf.image.crop_to_bounding_box(image, (nw - tw) // 2, (nh - th) // 2, tw, th)\n",
    "    return image, label\n",
    "\n",
    "batch_sizes = [2,4,6,8]\n",
    "batch_numbers = [3,6,12,18]\n",
    "repetitions=[1,2,3]\n",
    "\n",
    "parameter_list = [[i,j,k] for i in batch_sizes for j in batch_numbers for k in repetitions]\n",
    "\n",
    "#Task 2aii\n",
    "sc=pyspark.SparkContext.getOrCreate()\n",
    "RDD_parameter = sc.parallelize(parameter_list, 16) #creates an RDD\n",
    "\n",
    "def time_configs_combined(parameters_rdd):\n",
    "  batch_size, batch_count, repetition_count = parameters_rdd\n",
    "\n",
    "  filenames = tf.io.gfile.glob(GCS_PUBLIC_BUCKET + \"*.tfrec\")\n",
    "  dataset = load_dataset(filenames)\n",
    "\n",
    "  batched_dataset = dataset.batch(batch_size)\n",
    "  sample_dataset = batched_dataset.take(batch_count)\n",
    "\n",
    "  timings = []\n",
    "  for rep in range(repetition_count):\n",
    "    start_time = time.time()\n",
    "    for picture in sample_dataset:\n",
    "      print('string', file=open(\"/dev/null\", mode='w'))\n",
    "    end_time = time.time()\n",
    "    reading_duration = end_time - start_time\n",
    "    throughput = float((batch_size * batch_count) / reading_duration)\n",
    "    dataset_size = batch_size * batch_count\n",
    "    timings.append([batch_size, batch_count, repetition_count, dataset_size, throughput])\n",
    "  return timings\n",
    "\n",
    "#Task 2aiii\n",
    "tfrec_rdd = RDD_parameter.map(time_configs_combined)\n",
    "### TASK 2c ###\n",
    "tfrec_rdd.cache()\n",
    "\n",
    "#Task 2aiv\n",
    "# For TFRec dataset\n",
    "tfrec_images_per_second = tfrec_rdd.flatMap(lambda x: [(str(x[0][0])+'_'+str(x[0][1])+'_'+str(x[0][2]), x[0][4])])\n",
    "### TASK 2c ###\n",
    "tfrec_images_per_second.cache()\n",
    "tfrec_results = tfrec_images_per_second.collect()\n",
    "\n",
    "\n",
    "#Task 2av)\n",
    "# For TFRec dataset\n",
    "tfrec_images_per_second_avg = tfrec_images_per_second \\\n",
    "    .mapValues(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \\\n",
    "    .mapValues(lambda x: x[0] / x[1])\n",
    "tfrec_avg_results = tfrec_images_per_second_avg.collect()\n",
    "\n",
    "\n",
    "#Task 2avi)\n",
    "# Define your bucket name and destination file name\n",
    "import os\n",
    "PROJECT = os.environ.get(\"GCP_PROJECT\")\n",
    "destination_blob_name = \"average_results_tfrec.pkl\"\n",
    "\n",
    "def upload_to_bucket(BUCKET, destination_blob_name, data):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket('{}-storage'.format(PROJECT))\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    # Write the data to a temporary pickle file\n",
    "    with blob.open('wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "# Upload the results to the bucket\n",
    "upload_to_bucket(PROJECT, destination_blob_name, tfrec_avg_results)"
   ],
   "metadata": {
    "id": "SaLqEiZ2d0W7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ea5ab851-f8a4-4f51-c3dc-c33138537bb5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Decoded"
   ],
   "metadata": {
    "id": "0amZ2RIDtIbm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### CODING TASK\n",
    "#Task 2ai\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "\n",
    "GCS_PUBLIC_BUCKET = 'gs://flowers-public/*/*.jpg' # glob  pattern for input files\n",
    "\n",
    "def read_tfrecord(example):\n",
    "    features = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"class\": tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    # decode the TFRecord\n",
    "    example = tf.io.parse_single_example(example, features)\n",
    "    image = tf.image.decode_jpeg(example['image'], channels=3)\n",
    "    image = tf.reshape(image, [*TARGET_SIZE, 3])\n",
    "    class_num = example['class']\n",
    "    return image, class_num\n",
    "\n",
    "def load_dataset(filenames):\n",
    "    # read from TFRecords. For optimal performance, read from multiple\n",
    "    # TFRecord files at once and set the option experimental_deterministic = False\n",
    "    # to allow order-altering optimizations.\n",
    "    option_no_order = tf.data.Options()\n",
    "    option_no_order.experimental_deterministic = False\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.with_options(option_no_order)\n",
    "    dataset = dataset.map(read_tfrecord)\n",
    "    return dataset\n",
    "\n",
    "def load_dataset_decoded():\n",
    "  dataset_filename = tf.data.Dataset.list_files(GCS_PATTERN) #Image Files\n",
    "  datasetDecoded = dataset_filename.map(decode_jpeg_and_label)\n",
    "  datasetfn = datasetDecoded.map(resize_and_crop_image)\n",
    "  return datasetfn\n",
    "\n",
    "def decode_jpeg_and_label(filepath):\n",
    "    bits = tf.io.read_file(filepath)\n",
    "    image = tf.image.decode_jpeg(bits)\n",
    "    label = tf.strings.split(tf.expand_dims(filepath, axis=-1), sep='/')\n",
    "    label2 = label.values[-2]\n",
    "    return image, label2\n",
    "\n",
    "def resize_and_crop_image(input_layer):\n",
    "    image, label = input_layer\n",
    "    w = tf.shape(image)[0]\n",
    "    h = tf.shape(image)[1]\n",
    "    tw = TARGET_SIZE[1]\n",
    "    th = TARGET_SIZE[0]\n",
    "    resize_crit = (w * th) / (h * tw)\n",
    "    image = tf.cond(resize_crit < 1,\n",
    "                    lambda: tf.image.resize(image, [w*tw/w, h*tw/w]), # if true\n",
    "                    lambda: tf.image.resize(image, [w*th/h, h*th/h])  # if false\n",
    "                    )\n",
    "    nw = tf.shape(image)[0]\n",
    "    nh = tf.shape(image)[1]\n",
    "    image = tf.image.crop_to_bounding_box(image, (nw - tw) // 2, (nh - th) // 2, tw, th)\n",
    "    return image, label\n",
    "\n",
    "batch_sizes = [2,4,6,8]\n",
    "batch_numbers = [3,6,12,18]\n",
    "repetitions=[1,2,3]\n",
    "\n",
    "parameter_list = [[i,j,k] for i in batch_sizes for j in batch_numbers for k in repetitions]\n",
    "\n",
    "#Task 2aii\n",
    "sc=pyspark.SparkContext.getOrCreate()\n",
    "RDD_parameter = sc.parallelize(parameter_list) #creates an RDD\n",
    "\n",
    "def time_configs_combined(parameters):\n",
    "  batch_size, batch_count, repetition_count = parameters\n",
    "  filenames = tf.io.gfile.glob(GCS_PUBLIC_BUCKET + \"*.tfrec\")\n",
    "  dataset = load_dataset(filenames)\n",
    "\n",
    "  batched_dataset = dataset.batch(batch_size)\n",
    "  sample_dataset = batched_dataset.take(batch_count)\n",
    "\n",
    "  timings = []\n",
    "  for rep in range(repetition_count):\n",
    "    start_time = time.time()\n",
    "    for picture in sample_dataset:\n",
    "      print('string', file=open(\"/dev/null\", mode='w'))\n",
    "    end_time = time.time()\n",
    "    reading_duration = end_time - start_time\n",
    "    throughput = float((batch_size * batch_count) / reading_duration)\n",
    "    dataset_size = batch_size * batch_count\n",
    "    timings.append([batch_size, batch_count, repetition_count, dataset_size, throughput])\n",
    "  return timings\n",
    "\n",
    "#Task 2aiii\n",
    "tfrec_rdd = RDD_parameter.map(time_configs_combined)\n",
    "### TASK 2c ###\n",
    "tfrec_rdd.cache()\n",
    "\n",
    "#Task 2aiv\n",
    "# For TFRec dataset\n",
    "tfrec_images_per_second = tfrec_rdd.flatMap(lambda x: [(str(x[0][0])+'_'+str(x[0][1])+'_'+str(x[0][2]), x[0][4])])\n",
    "### TASK 2c ###\n",
    "tfrec_images_per_second.cache()\n",
    "tfrec_results = tfrec_images_per_second.collect()\n",
    "\n",
    "\n",
    "#Task 2av)\n",
    "# For TFRec dataset\n",
    "tfrec_images_per_second_avg = tfrec_images_per_second \\\n",
    "    .mapValues(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \\\n",
    "    .mapValues(lambda x: x[0] / x[1])\n",
    "tfrec_avg_results = tfrec_images_per_second_avg.collect()\n",
    "\n",
    "\n",
    "#Task 2avi)\n",
    "# Define your bucket name and destination file name\n",
    "import os\n",
    "PROJECT = os.environ.get(\"GCP_PROJECT\")\n",
    "destination_blob_name = \"average_results_decoded.pkl\"\n",
    "\n",
    "def upload_to_bucket(BUCKET, destination_blob_name, data):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket('{}-storage'.format(PROJECT))\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    # Write the data to a temporary pickle file\n",
    "    with blob.open('wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "# Upload the results to the bucket\n",
    "upload_to_bucket(PROJECT, destination_blob_name, tfrec_avg_results)"
   ],
   "metadata": {
    "id": "0NMBwceutFEt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Decoded File"
   ],
   "metadata": {
    "id": "4zKAvdGsC6To"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Task 2avii)\n",
    "\n",
    "%%writefile spark_job_decoded.py\n",
    "### CODING TASK\n",
    "#Task 2ai\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from google.cloud import storage\n",
    "import time\n",
    "\n",
    "GCS_PUBLIC_BUCKET = 'gs://flowers-public/*/*.jpg' # glob  pattern for input files\n",
    "PARTITIONS = 16 # no of partitions we will use later\n",
    "TARGET_SIZE = [192, 192] # target resolution for the images\n",
    "CLASSES = [b'daisy', b'dandelion', b'roses', b'sunflowers', b'tulips']\n",
    "    # labels for the data\n",
    "\n",
    "def read_tfrecord(example):\n",
    "    features = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"class\": tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    # decode the TFRecord\n",
    "    example = tf.io.parse_single_example(example, features)\n",
    "    image = tf.image.decode_jpeg(example['image'], channels=3)\n",
    "    image = tf.reshape(image, [*TARGET_SIZE, 3])\n",
    "    class_num = example['class']\n",
    "    return image, class_num\n",
    "\n",
    "def load_dataset(filenames):\n",
    "    # read from TFRecords. For optimal performance, read from multiple\n",
    "    # TFRecord files at once and set the option experimental_deterministic = False\n",
    "    # to allow order-altering optimizations.\n",
    "    option_no_order = tf.data.Options()\n",
    "    option_no_order.experimental_deterministic = False\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.with_options(option_no_order)\n",
    "    dataset = dataset.map(read_tfrecord)\n",
    "    return dataset\n",
    "\n",
    "def load_dataset_decoded():\n",
    "  dataset_filename = tf.data.Dataset.list_files(GCS_PATTERN) #Image Files\n",
    "  datasetDecoded = dataset_filename.map(decode_jpeg_and_label)\n",
    "  datasetfn = datasetDecoded.map(resize_and_crop_image)\n",
    "  return datasetfn\n",
    "\n",
    "def decode_jpeg_and_label(filepath):\n",
    "    bits = tf.io.read_file(filepath)\n",
    "    image = tf.image.decode_jpeg(bits)\n",
    "    label = tf.strings.split(tf.expand_dims(filepath, axis=-1), sep='/')\n",
    "    label2 = label.values[-2]\n",
    "    return image, label2\n",
    "\n",
    "def resize_and_crop_image(input_layer):\n",
    "    image, label = input_layer\n",
    "    w = tf.shape(image)[0]\n",
    "    h = tf.shape(image)[1]\n",
    "    tw = TARGET_SIZE[1]\n",
    "    th = TARGET_SIZE[0]\n",
    "    resize_crit = (w * th) / (h * tw)\n",
    "    image = tf.cond(resize_crit < 1,\n",
    "                    lambda: tf.image.resize(image, [w*tw/w, h*tw/w]), # if true\n",
    "                    lambda: tf.image.resize(image, [w*th/h, h*th/h])  # if false\n",
    "                    )\n",
    "    nw = tf.shape(image)[0]\n",
    "    nh = tf.shape(image)[1]\n",
    "    image = tf.image.crop_to_bounding_box(image, (nw - tw) // 2, (nh - th) // 2, tw, th)\n",
    "    return image, label\n",
    "\n",
    "batch_sizes = [2,4,6,8]\n",
    "batch_numbers = [3,6,12,18]\n",
    "repetitions=[1,2,3]\n",
    "\n",
    "parameter_list = [[i,j,k] for i in batch_sizes for j in batch_numbers for k in repetitions]\n",
    "\n",
    "#Task 2aii\n",
    "sc=pyspark.SparkContext.getOrCreate()\n",
    "RDD_parameter = sc.parallelize(parameter_list) #creates an RDD\n",
    "\n",
    "def time_configs_combined(parameters):\n",
    "  batch_size, batch_count, repetition_count = parameters\n",
    "  filenames = tf.io.gfile.glob(GCS_PUBLIC_BUCKET + \"*.tfrec\")\n",
    "  dataset = load_dataset(filenames)\n",
    "\n",
    "  batched_dataset = dataset.batch(batch_size)\n",
    "  sample_dataset = batched_dataset.take(batch_count)\n",
    "\n",
    "  timings = []\n",
    "  for rep in range(repetition_count):\n",
    "    start_time = time.time()\n",
    "    for picture in sample_dataset:\n",
    "      print('string', file=open(\"/dev/null\", mode='w'))\n",
    "    end_time = time.time()\n",
    "    reading_duration = end_time - start_time\n",
    "    throughput = float((batch_size * batch_count) / reading_duration)\n",
    "    dataset_size = batch_size * batch_count\n",
    "    timings.append([batch_size, batch_count, repetition_count, dataset_size, throughput])\n",
    "  return timings\n",
    "\n",
    "#Task 2aiii\n",
    "tfrec_rdd = RDD_parameter.map(time_configs_combined)\n",
    "### TASK 2c ###\n",
    "tfrec_rdd.cache()\n",
    "\n",
    "#Task 2aiv\n",
    "# For TFRec dataset\n",
    "tfrec_images_per_second = tfrec_rdd.flatMap(lambda x: [(str(x[0][0])+'_'+str(x[0][1])+'_'+str(x[0][2]), x[0][4])])\n",
    "### TASK 2c ###\n",
    "tfrec_images_per_second.cache()\n",
    "tfrec_results = tfrec_images_per_second.collect()\n",
    "\n",
    "\n",
    "#Task 2av)\n",
    "# For TFRec dataset\n",
    "tfrec_images_per_second_avg = tfrec_images_per_second \\\n",
    "    .mapValues(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \\\n",
    "    .mapValues(lambda x: x[0] / x[1])\n",
    "tfrec_avg_results = tfrec_images_per_second_avg.collect()\n",
    "\n",
    "\n",
    "#Task 2avi)\n",
    "# Define your bucket name and destination file name\n",
    "import os\n",
    "PROJECT = os.environ.get(\"GCP_PROJECT\")\n",
    "destination_blob_name = \"average_results_decoded.pkl\"\n",
    "\n",
    "def upload_to_bucket(BUCKET, destination_blob_name, data):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket('{}-storage'.format(PROJECT))\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    # Write the data to a temporary pickle file\n",
    "    with blob.open('wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "# Upload the results to the bucket\n",
    "upload_to_bucket(PROJECT, destination_blob_name, tfrec_avg_results)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vfHSL_gHC8XC",
    "outputId": "0e59556c-9777-467b-f7ae-53bfc2f67c9d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuXtPhZi2Ghp"
   },
   "source": [
    "### 2b) Testing the code and collecting results (4%)\n",
    "\n",
    "i) First, test locally with `%run`.\n",
    "\n",
    "It is useful to create a **new filename argument**, so that old results don't get overwritten.\n",
    "\n",
    "You can for instance use `datetime.datetime.now().strftime(\"%y%m%d-%H%M\")` to get a string with the current date and time and use that in the file name."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%run spark_job_tfrec.py"
   ],
   "metadata": {
    "id": "9uZVVYQBcGTr",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "56a7d394-b112-4838-dac2-8315a2056aef"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%run spark_job_decoded.py"
   ],
   "metadata": {
    "id": "uGSmBqKYCquC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01PowoXqAR9D"
   },
   "source": [
    "ii) Cloud\n",
    "\n",
    "If  you have a cluster running, you can run the  speed test job in the cloud.\n",
    "\n",
    "While you run this job, switch to the Dataproc web page and take **screenshots of the CPU and network load** over time. They are displayed with some delay, so you may need to wait a little. These images will be useful in the next task. Again, don't use the SCREENSHOT function that Google provides, but just take a picture of the graphs you see for the VMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "### CODING TASK ###\n",
    "CLUSTER = '{}-cluster'.format(PROJECT)\n",
    "!gcloud dataproc clusters create $CLUSTER \\\n",
    " --image-version 1.4-ubuntu18 \\\n",
    " --num-masters 1 \\\n",
    " --master-machine-type n1-standard-1 \\\n",
    " --master-boot-disk-type pd-ssd --master-boot-disk-size 100 \\\n",
    " --num-workers 7 \\\n",
    " --worker-machine-type n1-standard-1 \\\n",
    " --worker-boot-disk-size 285 \\\n",
    " --max-idle 3600s \\\n",
    " --initialization-actions gs://goog-dataproc-initialization-actions-$REGION/python/pip-install.sh \\\n",
    " --metadata PIP_PACKAGES=\"google-cloud-storage tensorflow==2.4.0\""
   ],
   "metadata": {
    "id": "KSNrC4Y18YaB",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9ee7ae38-5174-4c12-e27c-427a24ffd936"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!gcloud dataproc jobs submit pyspark --cluster $CLUSTER spark_job_tfrec.py"
   ],
   "metadata": {
    "id": "WnXEB4OrcAaE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "45c717a5-4e9d-4356-c9bd-81e793579ddd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!gcloud dataproc jobs submit pyspark --cluster $CLUSTER spark_job_decoded.py"
   ],
   "metadata": {
    "id": "_7yADux1FT5w",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "783dcdd6-af02-45ce-fec0-ced7140175bf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFVwxfjQsV_P"
   },
   "source": [
    "### 2c) Improve efficiency (6%)\n",
    "\n",
    "If you implemented a straightfoward version of 2a), you will **probably have an inefficiency** in your code.\n",
    "\n",
    "Because we are reading multiple times from an RDD to read the values for the different parameters and their averages, caching existing results is important. Explain **where in the process caching can help**, and **add a call to `RDD.cache()`** to your code, if you haven't yet. Measure the the effect of using caching or not using it.\n",
    "\n",
    "Make the **suitable change** in the code you have written above and mark them up in comments as `### TASK 2c ###`.\n",
    "\n",
    "Explain in your report what the **reasons for this change** are and **demonstrate and interpret its effect**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_uNa6VeTLSN"
   },
   "source": [
    "### 2d) Retrieve, analyse and discuss the output (12%)\n",
    "\n",
    "Run the tests over a wide range of different paramters and list the results in a table.\n",
    "\n",
    "Perform a **linear regression** (e.g. using scikit-learn) over **the values for each parameter** and for the **two cases** (reading from image files/reading TFRecord files).\n",
    "List a **table** with the output and interpret the results in terms of the effects of overall.  \n",
    "Also, **plot** the output values, the averages per parameter value and the regression lines for each parameter and for the product of batch_size and batch_number\n",
    "\n",
    "Discuss the **implications** of this result for **applications** like large-scale machine learning.\n",
    "Keep in mind that cloud data may be stored in distant physical locations.\n",
    "Use the numbers provided in the PDF latency-numbers document available on Moodle or [here](https://gist.github.com/hellerbarde/2843375) for your arguments.\n",
    "\n",
    "How is the **observed** behaviour **similar or different** from what you\u2019d expect from a **single machine**? Why would cloud providers tie throughput to capacity of disk resources?\n",
    "\n",
    "By **parallelising** the speed test we are making **assumptions** about the limits of the bucket reading speeds.\n",
    "See [here](https://cloud.google.com/storage/docs/request-rate) for more information.\n",
    "Discuss, **what we need to consider** in **speed tests** in parallel on the cloud, which bottlenecks we might be identifying, and how this relates to your results.\n",
    "\n",
    "Discuss to what extent **linear modelling** reflects the **effects** we are observing.\n",
    "Discuss what could be expected from a theoretical perspective and what can be useful in practice.\n",
    "  \n",
    "Write your **code below** and **include the output** in your submitted `ipynb` file. Provide the answer **text in your report**."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "### CODING TASK ###\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import scipy\n",
    "from matplotlib import pyplot as plt\n",
    "from google.cloud import storage\n",
    "import pickle\n",
    "\n",
    "def load_pickle(filename):\n",
    "    client = storage.Client()\n",
    "    bucket_name = \"{}-storage\".format(PROJECT)\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(filename)\n",
    "\n",
    "    with blob.open(\"rb\") as f:\n",
    "        file = pickle.load(f)\n",
    "\n",
    "    return file\n",
    "\n",
    "\n",
    "# Convert the parameter file to numeric values\n",
    "def get_numeric_params(pickle_entries):\n",
    "    numeric_params = []\n",
    "    for entry in pickle_entries:\n",
    "      combination = entry[0].split(\"_\")\n",
    "      combination = [int(param_value) for param_value in combination]\n",
    "      time = entry[1]\n",
    "      numeric_params.append([combination, time])\n",
    "\n",
    "    return numeric_params\n",
    "\n",
    "\n",
    "# Prepare the data for the regression\n",
    "tfrec_avg_results = load_pickle(\"average_results_tfrec.pkl\")\n",
    "decoded_avg_results = load_pickle(\"average_results_decoded.pkl\")\n",
    "\n",
    "tfrec_num = get_numeric_params(tfrec_avg_results)\n",
    "decoded_num = get_numeric_params(decoded_avg_results)\n",
    "\n",
    "def get_individual_parameter(array, name):\n",
    "    values = []\n",
    "    if name == \"batch_size\":\n",
    "        values = [value[0][0] for value in array]\n",
    "    elif name == \"batch_count\":\n",
    "        values = [value[0][1] for value in array]\n",
    "    elif name == \"repetitions_count\":\n",
    "        values = [value[0][2] for value in array]\n",
    "\n",
    "    time_values = [value[1] for value in array]\n",
    "\n",
    "    return values, time_values\n",
    "\n",
    "\n",
    "def plot_data(numeric_dataset, param_name, dataset_name):\n",
    "  param, time = get_individual_parameter(numeric_dataset, param_name)\n",
    "\n",
    "  plt.scatter(param, time)\n",
    "  slope, intercept, _, _, _ = scipy.stats.linregress(np.array(param), np.array(time))\n",
    "  line = intercept + slope * np.array(param)\n",
    "  plt.plot(param, line)\n",
    "  plt.xlabel(param_name, fontweight=\"bold\")\n",
    "  plt.ylabel(\"time\", fontweight=\"bold\")\n",
    "  plt.title(dataset_name)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plot_data(tfrec_num, \"batch_size\", \"TFRec - batch_size\")\n",
    "plot_data(tfrec_num, \"batch_count\", \"TFRec - batch_count\")\n",
    "plot_data(tfrec_num, \"repetitions_count\", \"TFRec - repetitions_count\")\n",
    "\n",
    "plot_data(decoded_num, \"batch_size\", \"Decoded - batch_size\")\n",
    "plot_data(decoded_num, \"batch_count\", \"Decoded - batch_count\")\n",
    "plot_data(decoded_num, \"repetitions_count\", \"Decoded - repetitions_count\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0GRVGs8nlp1h",
    "outputId": "604c9804-eaa1-4e70-f147-a69607547e58"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsvglpSzm78C"
   },
   "source": [
    "# Section 3. Theoretical discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYLysXrJm-9M"
   },
   "source": [
    "## Task 3: Discussion in context. (24%)\n",
    "\n",
    "In this task we refer an idea that is introduced in this paper:\n",
    "-\tAlipourfard, O., Liu, H. H., Chen, J., Venkataraman, S., Yu, M., & Zhang, M. (2017). [Cherrypick: Adaptively unearthing the best cloud configurations for big data analytics.](https://people.irisa.fr/Davide.Frey/wp-content/uploads/2018/02/cherrypick.pdf). In USENIX NSDI  17 (pp. 469-482).\n",
    "\n",
    "Alipourfard et al (2017) introduce  the prediction an optimal or near-optimal cloud configuration for a given compute task.\n",
    "\n",
    "### 3a)\tContextualise\n",
    "\n",
    "Relate the previous tasks and the results to this concept. (It is not necessary to work through the full details of the paper, focus just on the main ideas). To what extent and under what conditions do the concepts and techniques in the paper apply to the task in this coursework? (12%)\n",
    "\n",
    "### 3b)\tStrategise\n",
    "\n",
    "Define - as far as possible - concrete strategies for different application scenarios (batch, stream) and discuss the general relationship with the concepts above. (12%)\n",
    "\n",
    "Provide the answers to these questions in your report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srWp-rDKi61b"
   },
   "source": [
    "## Final cleanup\n",
    "\n",
    "Once you have finshed the work, you can delete the buckets, to stop incurring cost that depletes your credit."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ByR1KQ_SjG6W"
   },
   "source": [
    "!gsutil -m rm -r $BUCKET/* # Empty your bucket\n",
    "!gsutil rb $BUCKET # delete the bucket"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}